{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DCKRGL3WQeX"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# data handeling lirbaries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# plotting libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# model and evaluation libraries\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (\n",
        "    f1_score,\n",
        "    accuracy_score,\n",
        "    recall_score,\n",
        "    precision_score,\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        "    roc_auc_score,\n",
        "    precision_recall_curve,\n",
        "    roc_curve,\n",
        "    make_scorer,\n",
        "    average_precision_score,\n",
        ")\n",
        "\n",
        "# display settings for readability\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "pd.set_option(\"display.max_rows\", 200)\n",
        "pd.set_option(\"display.float_format\", lambda x: f\"{x:.5f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmWtBBuWlT7F"
      },
      "source": [
        "## Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukLrSofalVxM"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgSDINI5nKCm"
      },
      "outputs": [],
      "source": [
        "learn = pd.read_csv(\"/content/drive/MyDrive/Python/ExtraaLearn.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCUTg9EznMRP"
      },
      "outputs": [],
      "source": [
        "data = learn.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTvSS3dLhyAA"
      },
      "source": [
        "## Basic Overview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y01judBT1LTk"
      },
      "outputs": [],
      "source": [
        "print(\"Data shae:\", data.shape)\n",
        "print(\"Conversion rate:\", data[\"status\"].mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NV7wq_Vn1XU8"
      },
      "source": [
        "The dataset contains rows of leads and columns of features. The conversion rate is the proportion of leads with a positive outcome (status=1). This provides context on class imbalance for modelling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpInww_h5Vqc"
      },
      "outputs": [],
      "source": [
        "# helper function for visualization\n",
        "\n",
        "def histogram_boxplot(df, feature, figsize=(12, 7), kde=False, bins=\"auto\"):\n",
        "\n",
        "    #\n",
        "    mean_val = df[feature].mean()\n",
        "    med_val = df[feature].median()\n",
        "\n",
        "    # set up the figure with two parts: small top, big bottom\n",
        "    fig, (ax_box, ax_hist) = plt.subplots(\n",
        "        nrows=2,\n",
        "        sharex=True,\n",
        "        gridspec_kw={\"height_ratios\": (0.25, 0.75)},\n",
        "        figsize=figsize\n",
        "    )\n",
        "\n",
        "    # boxplot\n",
        "    sns.boxplot(x=df[feature], ax=ax_box, showmeans=True, color=\"violet\")\n",
        "    ax_box.set_title(f\"Distribution of {feature}\")\n",
        "    ax_box.set(xlabel='') # hide x-label on the top plot\n",
        "\n",
        "    # histogram\n",
        "    sns.histplot(df[feature], kde=kde, ax=ax_hist, bins=bins, stat=\"density\")\n",
        "\n",
        "    # add refernece lines\n",
        "    ax_hist.axvline(mean_val, color=\"green\", linestyle=\"--\", label='Mean')\n",
        "    ax_hist.axvline(med_val, color=\"black\", linestyle=\"-\", label='Median')\n",
        "    ax_hist.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def labeled_barplot(df, feature, perc=False, n=None):\n",
        "    \"\"\"Plots a bar chart. If perc=True, plots percentages instead of counts.\"\"\"\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # get the data ready before plotting\n",
        "    data_series = df[feature].value_counts()\n",
        "\n",
        "    if n:\n",
        "        data_series = data_series.head(n)\n",
        "\n",
        "    # convert the data\n",
        "    if perc:\n",
        "        total = len(df)\n",
        "        # convert counts to percentage of total\n",
        "        data_series = (data_series / total) * 100\n",
        "        y_label = \"Percentage\"\n",
        "        fmt_str = '%.1f%%'\n",
        "    else:\n",
        "        y_label = \"Count\"\n",
        "        fmt_str = '%.0f'\n",
        "\n",
        "\n",
        "    #\n",
        "    ax = sns.barplot(x=data_series.index, y=data_series.values, palette=\"Paired\")\n",
        "\n",
        "    # add labels using modern matplotlib\n",
        "    ax.bar_label(ax.containers[0], fmt=fmt_str, padding=3)\n",
        "\n",
        "    plt.ylabel(y_label)\n",
        "    plt.xticks(rotation=45) \n",
        "    plt.title(f\"{feature} - {y_label}\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMnNN5hWx9Zu"
      },
      "source": [
        "## Exploratory Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WE3xTVBvyA6Z"
      },
      "outputs": [],
      "source": [
        "# Drop the ID column and preserve lead IDs separately for ranking output later\n",
        "lead_ids = data['ID']\n",
        "data.drop(['ID'], axis=1, inplace=True)\n",
        "\n",
        "# Numeric features of interest\n",
        "numeric_features = [\"age\", \"website_visits\", \"time_spent_on_website\", \"page_views_per_visit\"]\n",
        "\n",
        "for feature in numeric_features:\n",
        "    histogram_boxplot(data, feature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stWbkpLS8U2n"
      },
      "source": [
        "### Observations from Numeric Features\n",
        "- Longer time spent on the website tends to associate with higher conversion.\n",
        "- Other numerical features show less pronounced separation between converters and nonâ€‘converters but are included for completeness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4EpS7EF7ZSb"
      },
      "outputs": [],
      "source": [
        "# Selected categorical features for barplots\n",
        "categorical_features = [\"current_occupation\", \"first_interaction\", \"profile_completed\", \"referral\"]\n",
        "\n",
        "for feature in categorical_features:\n",
        "    labeled_barplot(data, feature, perc=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncs58Pgm8gq3"
      },
      "source": [
        "### Observations from Categoritcal Features\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40LE6YUn8yId"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dr7UxNQ8wkp"
      },
      "outputs": [],
      "source": [
        "X = data.drop(\"status\", axis=1)\n",
        "y = data[\"status\"]\n",
        "\n",
        "# categorical variable\n",
        "X = pd.get_dummies(X, drop_first=True)\n",
        "\n",
        "print(f\"Encoded features: {X.columns.tolist()[:5]}...\")\n",
        "\n",
        "# split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=1, stratify=y\n",
        ")\n",
        "\n",
        "print(\"Train size:\", X_train.shape)\n",
        "print(\"Test size:\", X_test.shape)\n",
        "print(\"\\nTarget balance (normalized):\")\n",
        "print(y_train.value_counts(normalize=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CogOS3VHGVLi"
      },
      "outputs": [],
      "source": [
        "dt = DecisionTreeClassifier(random_state=7, class_weight='balanced')\n",
        "dt.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiOmesAWGaZA"
      },
      "outputs": [],
      "source": [
        "print(\"Training Performance: \")\n",
        "train_pred_dt = dt.predict(X_train)\n",
        "print(classification_report(y_train, train_pred_dt))\n",
        "\n",
        "print(\"Test Performance: \")\n",
        "test_pred_dt = dt.predict(X_test)\n",
        "print(classification_report(y_test, test_pred_dt))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cHwpL0n4mHk"
      },
      "outputs": [],
      "source": [
        "# confusion matrix for test results\n",
        "cm = confusion_matrix(y_test, test_pred_dt)\n",
        "\n",
        "plt.figure(figsize=(7, 5))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Decision Tree: Test Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plcdtThAKE3M"
      },
      "outputs": [],
      "source": [
        "dt_tuned = DecisionTreeClassifier(random_state=7, class_weight='balanced')\n",
        "\n",
        "dt_param_grid = {\n",
        "    'max_depth': [3, 4, 5, 7, 10],\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'min_samples_leaf': [5, 10, 20]\n",
        "}\n",
        "\n",
        "# grid search\n",
        "dt_grid = GridSearchCV(dt_tuned, dt_param_grid, scoring='recall', cv=5, n_jobs=-1)\n",
        "dt_grid.fit(X_train, y_train)\n",
        "\n",
        "dt_best = dt_grid.best_estimator_\n",
        "\n",
        "print(\"Best Parameters found:\", dt_grid.best_params_)\n",
        "print(f\"Best Recall Score (CV): {dt_grid.best_score_:.3f}\")\n",
        "\n",
        "print(\"\\n--- Tuned Decision Tree Performance (Test) ---\")\n",
        "test_pred_dt_best = dt_best.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, test_pred_dt_best))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VazGVbyKQBR"
      },
      "outputs": [],
      "source": [
        "# Random forest model\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=7, class_weight='balanced')\n",
        "\n",
        "# fit the model\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "print(\"--- Random Forest: Training Set ---\")\n",
        "train_pred_rf = rf.predict(X_train)\n",
        "\n",
        "print(classification_report(y_train, train_pred_rf))\n",
        "\n",
        "# check test performance\n",
        "print(\"\\n--- Random Forest: Test Set ---\")\n",
        "test_pred_rf = rf.predict(X_test)\n",
        "print(classification_report(y_test, test_pred_rf))\n",
        "\n",
        "import pandas as pd\n",
        "feat_importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
        "print(\"\\nTop 5 Important Features:\")\n",
        "print(feat_importances.nlargest(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PghPqZbCLsGI"
      },
      "outputs": [],
      "source": [
        "# Hyperparameter tuning for random forest\n",
        "rf_tuned = RandomForestClassifier(random_state=7)\n",
        "\n",
        "rf_param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [5, 10, 15],\n",
        "    'min_samples_leaf': [10, 20, 50],\n",
        "    'class_weight': ['balanced', 'balanced_subsample']\n",
        "}\n",
        "\n",
        "# grid search\n",
        "rf_grid = GridSearchCV(rf_tuned, rf_param_grid, scoring='recall', cv=5, n_jobs=-1)\n",
        "rf_grid.fit(X_train, y_train)\n",
        "\n",
        "rf_best = rf_grid.best_estimator_\n",
        "\n",
        "print(\"Best Parameters Found:\", rf_grid.best_params_)\n",
        "print(f\"Best Recall Score: {rf_grid.best_score_:.3f}\")\n",
        "\n",
        "print(\"\\n--- Tuned Random Forest Performance (Test) ---\")\n",
        "test_pred_rf_best = rf_best.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, test_pred_rf_best))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AifIRgjALtEo"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BD16YS0ILvGl"
      },
      "outputs": [],
      "source": [
        "# Ranking evaluation and feature importance for random forest\n",
        "\n",
        "y_train_probs = rf_best.predict_proba(X_train)[:, 1]\n",
        "y_test_probs = rf_best.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"--- Ranking Metrics (Random Forest) ---\")\n",
        "\n",
        "print(f\"ROC AUC (Train): {roc_auc_score(y_train, y_train_probs):.3f}\")\n",
        "print(f\"ROC AUC (Test):  {roc_auc_score(y_test, y_test_probs):.3f}\")\n",
        "print(f\"Average Precision (Test): {average_precision_score(y_test, y_test_probs):.3f}\")\n",
        "\n",
        "top_frac = 0.2\n",
        "n_top = int(len(y_test) * top_frac)\n",
        "\n",
        "# get indices of the highest scoring leads\n",
        "top_indices = np.argsort(y_test_probs)[-n_top:]\n",
        "\n",
        "# calculate conversion rate for just these leads\n",
        "top_conversion = y_test.iloc[top_indices].mean()\n",
        "\n",
        "print(f\"\\nBusiness Check: Conversion Rate in Top {int(top_frac*100)}% of Leads: {top_conversion:.1%}\")\n",
        "\n",
        "# feature importance\n",
        "feat_imp_df = pd.DataFrame({\n",
        "    'feature': X_train.columns,\n",
        "    'importance': rf_best.feature_importances_\n",
        "})\n",
        "\n",
        "# sort\n",
        "feat_imp_df = feat_imp_df.sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"\\nTop 10 Drivers:\")\n",
        "print(feat_imp_df.head(10))\n",
        "\n",
        "# plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "sns.barplot(y='feature', x='importance', data=feat_imp_df.head(15), color='steelblue')\n",
        "plt.title('Feature Importance (Random Forest)')\n",
        "plt.xlabel('Importance Score')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmuWvEvcOD4B"
      },
      "outputs": [],
      "source": [
        "\n",
        "full_scores = rf_best.predict_proba(X)[:, 1]\n",
        "\n",
        "ranked_leads = pd.DataFrame({\n",
        "    'lead_id': lead_ids, # Defined in the very first step\n",
        "    'conversion_score': full_scores\n",
        "})\n",
        "\n",
        "# Sort high to low so the sales team sees the best leads first\n",
        "ranked_leads = ranked_leads.sort_values('conversion_score', ascending=False)\n",
        "\n",
        "# save to csv\n",
        "ranked_leads.to_csv('ranked_leads.csv', index=False)\n",
        "\n",
        "print(\"Success! Ranked leads saved to 'ranked_leads.csv'. Preview:\")\n",
        "print(ranked_leads.head(10))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
